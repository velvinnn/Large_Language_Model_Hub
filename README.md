# Large_Language_Model_Hub
Papers, tutorials, code for large language models

# Table of Contents
1. [Key Language Models](#key-language-models)
2. [Training and Optimization](#training-and-optimization)
3. [Benchmarks and datasets](#benchmarks-and-datasets)
4. [Prompt-tuning](#prompt-tuning)
5. [ChatGPT / GPT-4](#chatgpt--gpt-4)
6. [Augmented LLM](#augmented-llm)
7. [Retrieval-base NLP](#retrieval-base-nlp)
8. [Emergent capabilities](#emergent-capabilities)
9. [Clinical Applications](#clinical-applications)
10. [Other github resources](#other-github-resources)
11. [Discussions](#discussions)

## Key language models
- [A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT](https://arxiv.org/pdf/2302.09419.pdf)
- [A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT](https://arxiv.org/abs/2303.04226)
- [GPT3: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
  - [Stanford Webinar - GPT-3 & Beyond](https://www.youtube.com/watch?v=-lnHHWRCDGk) ([Slides](https://docs.google.com/presentation/d/1WPYaLEEVJJI_-DOzjudeVoYpl_y0yUi1kWs0VFBnba4/edit#slide=id.g1c79e641885_1_554))
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) 
  - [Vedio with Paper Explained](https://www.youtube.com/watch?v=E5OnoYF2oAk)
  - [LLaMA-Adapter: Efficient Fine-tuning of LLaMA](https://github.com/ZrrSkywalker/LLaMA-Adapter)
- [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311)
  - [Pathways Language Model and Model Scaling - Aakanksha Chowdhery | Stanford MLSys #69](https://www.youtube.com/watch?v=CV_eBVwzOaw)
- [FLAN: Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)
- [Scaling Instruction-Finetuned Language Models (Flan-PaLM)] (https://arxiv.org/abs/2210.11416)
  - Paper explained [video]([https://arxiv.org/abs/2210.11416](https://www.youtube.com/watch?v=QdwETwqyREY), [slides](https://samuelalbanie.com/files/digest-slides/2022-10-scaling-instruction-finetuned-language-models.pdf)
- 

## Training and optimization
- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)
  - [Explanation vedio: How ChatGPT is trained](https://www.youtube.com/watch?v=VPRSBzXzavo)
- [The Large Language Model Training Playbook](https://github.com/huggingface/large_language_model_training_playbook)
- [Scaling Expert Language Models with Unsupervised Domain Discovery](https://arxiv.org/abs/2303.14177)
- [Summary of open-source APIs, tools and services for LLM](https://github.com/kasperjunge/LLM-Guide)

## Benchmarks and datasets
- [Beyond the Imitation Game benchmark (BIG-bench)](https://arxiv.org/abs/2206.04615)
- [Measuring Massive Multitask Language Understanding (MMLU Dataset)](https://arxiv.org/abs/2009.03300)

## Prompt-tuning
- [The Art of Asking ChatGPT for High-Quality Answers: A complete Guide to Prompt-Engineering Technique](https://www.amazon.com/Art-Asking-ChatGPT-High-Quality-Answers/dp/B0BT2JB67Y)
- [Prompt-Engineering-Guide](https://github.com/dair-ai/Prompt-Engineering-Guide)
  - [1-hour tutorial](https://www.youtube.com/watch?v=dOxUroR57xs) ([Slides](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/lecture/Prompt-Engineering-Lecture-Elvis.pdf))

## ChatGPT / GPT-4
- [GPT-4 Technical Report](https://cdn.openai.com/papers/gpt-4.pdf)
- [Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712v1)
- [GPT-4 developer demo livestream](https://www.youtube.com/watch?v=outcGtbnMuQ&ab_channel=OpenAI)

## Augmented LLM
- [Augmented Language Models: a Survey](https://arxiv.org/abs/2302.07842)
- [Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback](https://arxiv.org/abs/2302.12813)
- [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/abs/2302.04761)

## Retrieval-base NLP
- [Memorizing Transformers](https://arxiv.org/abs/2203.08913)
- [Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP](https://arxiv.org/abs/2212.14024)

## Emergent capabilities
- [Emergent abilities of large language models](https://www.jasonwei.net/blog/emergence)
- [Capabilities of GPT-4 on Medical Challenge Problems](https://arxiv.org/abs/2303.13375)
- [Large AI Models in Health Informatics: Applications, Challenges, and the Future](https://arxiv.org/abs/2303.11568)
- [Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9931230/)
- [Zero-shot Clinical Entity Recognition using ChatGPT](https://arxiv.org/pdf/2303.16416v1.pdf)
- [ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks](https://arxiv.org/abs/2303.15056)

## Clinical applications
- [Open source diagnosis generator](https://glass.health/ai)
- [ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge](https://arxiv.org/abs/2303.14070v1)
- [Med-PaLM](Large Language Models Encode Clinical Knowledge)
- [Clinical-T5: Large Language Models Built Using MIMIC Clinical Text](https://physionet.org/content/clinical-t5/1.0.0/)
- 

## Other github resources
- [Reasoning in Large Language Models](https://github.com/jeffhj/LM-reasoning)

## Discussions
- [Yann LeCun: do large language models need sensory grounding for meaning and understanding](https://drive.google.com/file/d/1BU5bV3X5w65DwSMapKcsr0ZvrMRU_Nbi/view)
- [The Low-Rank Simplicity Bias in Deep Networks](https://arxiv.org/abs/2103.10427)
